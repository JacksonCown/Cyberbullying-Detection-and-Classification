{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "#import seaborn as sns\n",
    "#import transformers\n",
    "#import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import time \n",
    "\n",
    "# Check GPU Utility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = pd.read_csv('data/cleaned/nohashtag_cleaned_lemmatized_english.csv')\n",
    "#tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>word food crapilicious</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>katandandre mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aussietv MKR theblock ImACelebrityAU today sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@username classy whore red velvet cupcake</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>XochitlSuckkks</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@username meh p thanks head concern angry dude...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Jason_Gio</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@username isi account pretend kurdish account ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>RudhoeEnglish</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0                             word food crapilicious   \n",
       "1           1                                              white   \n",
       "2           2          @username classy whore red velvet cupcake   \n",
       "3           3  @username meh p thanks head concern angry dude...   \n",
       "4           4  @username isi account pretend kurdish account ...   \n",
       "\n",
       "   cyberbullying_type lang mentioned_users  \\\n",
       "0                   0   en             NaN   \n",
       "1                   0   en             NaN   \n",
       "2                   0   en  XochitlSuckkks   \n",
       "3                   0   en       Jason_Gio   \n",
       "4                   0   en   RudhoeEnglish   \n",
       "\n",
       "                                            hashtags  \n",
       "0                                    katandandre mkr  \n",
       "1  aussietv MKR theblock ImACelebrityAU today sun...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old mapping, keeping for now unless\n",
    "#mapping = {'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4, 'other_cyberbullying':5}\n",
    "\n",
    "\n",
    "tweets = pd.read_csv('data/cleaned/nohashtag_cleaned_lemmatized_english.csv')\n",
    "\n",
    "\n",
    "label_to_int = {\"not_cyberbullying\":0,\n",
    "           \"religion\":1,\n",
    "           \"age\":2,\n",
    "           \"gender\":3,\n",
    "           \"ethnicity\":4,\n",
    "           \"other_cyberbullying\":5}\n",
    "\n",
    "def map_label(label):\n",
    "    return label_to_int[label]\n",
    "\n",
    "#tweets = lem_english#[['tweet_text','cyberbullying_type']]\n",
    "tweets['cyberbullying_type'] = tweets['cyberbullying_type'].apply(map_label)\n",
    "\n",
    "tweets['tweet_text'] = tweets['tweet_text'].fillna('<UNK>')\n",
    "tweets['tweet_text'] = tweets['tweet_text'].replace('nan', '<UNK>')\n",
    "\n",
    "tweets_x = tweets['tweet_text'].values\n",
    "tweets_y = tweets['cyberbullying_type'].values\n",
    "\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 4781],\n",
       "       [   1, 4781],\n",
       "       [   2, 4781],\n",
       "       [   3, 4781],\n",
       "       [   4, 4781],\n",
       "       [   5, 4781]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets_x, tweets_y, test_size=0.2, stratify=tweets_y, random_state=20)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=20)\n",
    "\n",
    "ran_overSamp = RandomOverSampler()\n",
    "\n",
    "X_train_os, y_train_os = ran_overSamp.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))\n",
    "\n",
    "X_train_os = X_train_os.flatten()\n",
    "y_train_os = y_train_os.flatten()\n",
    "\n",
    "(unique, counts) = np.unique(y_train_os, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  512\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True, truncation=True, max_length=MAX_LEN) for sent in X_train]\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_tokenizer(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
    "            max_length=MAX_LEN,             # Choose max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length \n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = roberta_tokenizer(X_train_os)\n",
    "val_inputs, val_masks = roberta_tokenizer(X_valid)\n",
    "test_inputs, test_masks = roberta_tokenizer(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.from_numpy(y_train_os)\n",
    "val_labels = torch.from_numpy(y_valid)\n",
    "test_labels = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class Roberta_Classifier(nn.Module):\n",
    "    def __init__(self, freeze_roberta=False):\n",
    "        super(Roberta_Classifier, self).__init__()\n",
    "        # Specify hidden size of RoBERTa, hidden size of the classifier, and number of labels\n",
    "        n_input = 768\n",
    "        n_hidden = 50\n",
    "        n_output = 6\n",
    "        # Instantiate RoBERTa model\n",
    "        self.roberta = RobertaModel.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "        # Add dense layers to perform the classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_input,  n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "        # Add possibility to freeze the RoBERTa model\n",
    "        # to avoid fine tuning RoBERTa params (usually leads to worse results)\n",
    "        if freeze_roberta:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input data to BERT\n",
    "        outputs = self.roberta(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    # Instantiate RoBERTa Classifier\n",
    "    roberta_classifier = Roberta_Classifier(freeze_roberta=False)\n",
    "    \n",
    "    roberta_classifier.to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(roberta_classifier.parameters(),\n",
    "                      lr=5e-5,    # learning rate, set to default value\n",
    "                      eps=1e-8    # decay, set to default value\n",
    "                      )\n",
    "    \n",
    "    ### Set up learning rate scheduler ###\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Defint the scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return roberta_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "roberta_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def roberta_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune RoBERTa params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        # Put the model into the evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define empty lists to host accuracy and validation for each batch\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # We do not want to update the params during the evaluation,\n",
    "            # So we specify that we dont want to compute the gradients of the tensors\n",
    "            # by calling the torch.no_grad() method\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions starting from the logits (get index of highest logit)\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the validation accuracy \n",
    "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "        \n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\"*61)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(roberta_classifier, val_dataloader):\n",
    "    pred_vec = np.zeros((len(val_dataloader), 5))\n",
    "    label_vec = np.zeros((len(val_dataloader), 5))\n",
    "    for batch in val_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "        # Get the predictions starting from the logits (get index of highest logit)\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        \n",
    "    return preds, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "----------\n",
      "Epoch : 1\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   1.790853   |   18.01  \n",
      "   200    |   1.674218   |   15.01  \n",
      "   300    |   1.383587   |   14.98  \n",
      "   400    |   1.014792   |   15.02  \n",
      "   500    |   0.807930   |   14.99  \n",
      "   600    |   0.783020   |   15.01  \n",
      "   700    |   0.645348   |   15.01  \n",
      "   800    |   0.687578   |   15.02  \n",
      "   900    |   0.606161   |   15.03  \n",
      "  1000    |   0.900756   |   15.15  \n",
      "  1100    |   0.575013   |   15.20  \n",
      "  1200    |   0.649389   |   15.10  \n",
      "  1300    |   0.745383   |   15.15  \n",
      "  1400    |   0.636626   |   14.97  \n",
      "  1500    |   0.658923   |   15.00  \n",
      "  1600    |   0.653201   |   15.02  \n",
      "  1700    |   0.666398   |   15.03  \n",
      "  1800    |   0.696570   |   15.23  \n",
      "  1900    |   0.652333   |   16.07  \n",
      "  2000    |   0.621738   |   16.90  \n",
      "  2100    |   0.607865   |   15.76  \n",
      "  2200    |   0.580013   |   16.82  \n",
      "  2300    |   0.657416   |   16.24  \n",
      "  2400    |   0.718999   |   15.75  \n",
      "  2500    |   0.539008   |   15.23  \n",
      "  2600    |   0.556916   |   15.77  \n",
      "  2700    |   0.517338   |   15.34  \n",
      "  2800    |   0.706462   |   15.39  \n",
      "  2900    |   0.543158   |   15.40  \n",
      "  3000    |   0.572119   |   16.03  \n",
      "  3100    |   0.630032   |   16.49  \n",
      "  3200    |   0.547745   |   15.52  \n",
      "  3300    |   0.623419   |   15.49  \n",
      "  3400    |   0.564490   |   16.78  \n",
      "  3500    |   0.583580   |   16.63  \n",
      "  3600    |   0.645052   |   16.67  \n",
      "  3700    |   0.606515   |   16.68  \n",
      "  3800    |   0.673436   |   16.59  \n",
      "  3900    |   0.578344   |   16.66  \n",
      "  4000    |   0.569761   |   16.58  \n",
      "  4100    |   0.613713   |   16.59  \n",
      "  4200    |   0.632487   |   16.60  \n",
      "  4300    |   0.627297   |   16.60  \n",
      "  4400    |   0.584428   |   16.59  \n",
      "  4500    |   0.584298   |   16.59  \n",
      "  4600    |   0.588877   |   16.58  \n",
      "  4700    |   0.572250   |   16.58  \n",
      "  4800    |   0.761145   |   16.59  \n",
      "  4900    |   0.575404   |   16.61  \n",
      "  5000    |   0.623344   |   16.59  \n",
      "  5100    |   0.610163   |   16.57  \n",
      "  5200    |   0.656269   |   16.56  \n",
      "  5300    |   0.648015   |   16.57  \n",
      "  5400    |   0.542288   |   16.59  \n",
      "  5500    |   0.550523   |   16.71  \n",
      "  5600    |   0.543009   |   16.62  \n",
      "  5700    |   0.510267   |   17.19  \n",
      "  5737    |   0.459879   |   6.09   \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.684617    |  0.571407  |       78.04       |  999.30  \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 2\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.569346   |   18.06  \n",
      "   200    |   0.601131   |   21.36  \n",
      "   300    |   0.591594   |   18.44  \n",
      "   400    |   0.586409   |   18.10  \n",
      "   500    |   0.565648   |   17.90  \n",
      "   600    |   0.598261   |   17.61  \n",
      "   700    |   0.516613   |   17.79  \n",
      "   800    |   0.503308   |   17.96  \n",
      "   900    |   0.674778   |   18.06  \n",
      "  1000    |   0.475396   |   17.54  \n",
      "  1100    |   0.648215   |   17.68  \n",
      "  1200    |   0.540416   |   17.86  \n",
      "  1300    |   0.595086   |   17.83  \n",
      "  1400    |   0.472891   |   17.46  \n",
      "  1500    |   0.603138   |   18.53  \n",
      "  1600    |   0.577030   |   18.14  \n",
      "  1700    |   0.468108   |   18.00  \n",
      "  1800    |   0.544713   |   18.14  \n",
      "  1900    |   0.558707   |   18.36  \n",
      "  2000    |   0.536273   |   18.37  \n",
      "  2100    |   0.608466   |   18.01  \n",
      "  2200    |   0.522664   |   18.03  \n",
      "  2300    |   0.546419   |   18.22  \n",
      "  2400    |   0.502198   |   18.48  \n",
      "  2500    |   0.554487   |   18.02  \n",
      "  2600    |   0.498638   |   16.26  \n",
      "  2700    |   0.562695   |   18.30  \n",
      "  2800    |   0.657973   |   16.82  \n",
      "  2900    |   0.562950   |   18.29  \n",
      "  3000    |   0.466645   |   17.25  \n",
      "  3100    |   0.513375   |   16.30  \n",
      "  3200    |   0.545298   |   16.23  \n",
      "  3300    |   0.585416   |   16.43  \n",
      "  3400    |   0.649110   |   16.62  \n",
      "  3500    |   0.570964   |   16.59  \n",
      "  3600    |   0.578597   |   16.58  \n",
      "  3700    |   0.514104   |   16.56  \n",
      "  3800    |   0.442032   |   16.62  \n",
      "  3900    |   0.632356   |   16.66  \n",
      "  4000    |   0.515688   |   16.60  \n",
      "  4100    |   0.595978   |   16.57  \n",
      "  4200    |   0.531127   |   16.53  \n",
      "  4300    |   0.564928   |   16.61  \n",
      "  4400    |   0.554241   |   16.57  \n",
      "  4500    |   0.590795   |   16.57  \n",
      "  4600    |   0.530403   |   16.59  \n",
      "  4700    |   0.594358   |   16.59  \n",
      "  4800    |   0.511919   |   16.61  \n",
      "  4900    |   0.510320   |   16.66  \n",
      "  5000    |   0.554528   |   16.62  \n",
      "  5100    |   0.518391   |   16.61  \n",
      "  5200    |   0.547079   |   16.61  \n",
      "  5300    |   0.567608   |   16.66  \n",
      "  5400    |   0.493765   |   16.61  \n",
      "  5500    |   0.508119   |   16.86  \n",
      "  5600    |   0.555612   |   16.60  \n",
      "  5700    |   0.481407   |   16.51  \n",
      "  5737    |   0.567871   |   6.11   \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.552170    |  0.485463  |       81.84       |  1071.58 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 3\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.435733   |   16.57  \n",
      "   200    |   0.464292   |   16.49  \n",
      "   300    |   0.452815   |   16.48  \n",
      "   400    |   0.499355   |   16.67  \n",
      "   500    |   0.385489   |   16.57  \n",
      "   600    |   0.384132   |   16.65  \n",
      "   700    |   0.580771   |   16.78  \n",
      "   800    |   0.498410   |   16.86  \n",
      "   900    |   0.588179   |   16.49  \n",
      "  1000    |   0.501603   |   16.50  \n",
      "  1100    |   0.514261   |   16.51  \n",
      "  1200    |   0.517280   |   16.50  \n",
      "  1300    |   0.401156   |   16.92  \n",
      "  1400    |   0.498805   |   16.47  \n",
      "  1500    |   0.495652   |   16.42  \n",
      "  1600    |   0.500645   |   16.41  \n",
      "  1700    |   0.503421   |   16.44  \n",
      "  1800    |   0.499032   |   16.43  \n",
      "  1900    |   0.480774   |   16.45  \n",
      "  2000    |   0.423187   |   16.44  \n",
      "  2100    |   0.519040   |   17.17  \n",
      "  2200    |   0.483958   |   17.00  \n",
      "  2300    |   0.450062   |   16.87  \n",
      "  2400    |   0.481216   |   16.92  \n",
      "  2500    |   0.503447   |   16.63  \n",
      "  2600    |   0.425759   |   16.57  \n",
      "  2700    |   0.495447   |   16.61  \n",
      "  2800    |   0.450796   |   16.61  \n",
      "  2900    |   0.457654   |   16.57  \n",
      "  3000    |   0.502282   |   16.60  \n",
      "  3100    |   0.440170   |   16.63  \n",
      "  3200    |   0.535566   |   16.62  \n",
      "  3300    |   0.526218   |   16.61  \n",
      "  3400    |   0.535877   |   16.60  \n",
      "  3500    |   0.596740   |   16.56  \n",
      "  3600    |   0.549056   |   16.57  \n",
      "  3700    |   0.529673   |   16.61  \n",
      "  3800    |   0.506126   |   16.55  \n",
      "  3900    |   0.399436   |   16.60  \n",
      "  4000    |   0.510235   |   16.67  \n",
      "  4100    |   0.497301   |   16.63  \n",
      "  4200    |   0.443474   |   16.60  \n",
      "  4300    |   0.501653   |   16.71  \n",
      "  4400    |   0.497251   |   16.62  \n",
      "  4500    |   0.460345   |   16.63  \n",
      "  4600    |   0.435683   |   16.61  \n",
      "  4700    |   0.449018   |   16.60  \n",
      "  4800    |   0.466242   |   16.62  \n",
      "  4900    |   0.524884   |   16.63  \n",
      "  5000    |   0.435644   |   16.60  \n",
      "  5100    |   0.568290   |   16.97  \n",
      "  5200    |   0.491589   |   16.51  \n",
      "  5300    |   0.531278   |   16.53  \n",
      "  5400    |   0.536664   |   16.55  \n",
      "  5500    |   0.483428   |   16.58  \n",
      "  5600    |   0.502604   |   16.60  \n",
      "  5700    |   0.466859   |   16.63  \n",
      "  5737    |   0.531826   |   6.13   \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.488273    |  0.488061  |       83.36       |  1030.55 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 4\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n",
      "   100    |   0.504586   |   16.72  \n",
      "   200    |   0.426455   |   16.63  \n",
      "   300    |   0.396311   |   16.59  \n",
      "   400    |   0.523578   |   16.58  \n",
      "   500    |   0.524628   |   16.63  \n",
      "   600    |   0.435329   |   16.66  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   700    |   0.349279   |   16.56  \n",
      "   800    |   0.525785   |   16.52  \n",
      "   900    |   0.448333   |   16.51  \n",
      "  1000    |   0.476736   |   16.46  \n",
      "  1100    |   0.463183   |   16.50  \n",
      "  1200    |   0.433211   |   16.50  \n",
      "  1300    |   0.519769   |   16.55  \n",
      "  1400    |   0.499458   |   16.65  \n",
      "  1500    |   0.490348   |   16.58  \n",
      "  1600    |   0.464520   |   16.88  \n",
      "  1700    |   0.497374   |   16.55  \n",
      "  1800    |   0.461478   |   16.60  \n",
      "  1900    |   0.390176   |   16.51  \n",
      "  2000    |   0.460366   |   16.51  \n",
      "  2100    |   0.419902   |   16.50  \n",
      "  2200    |   0.395214   |   16.53  \n",
      "  2300    |   0.390720   |   16.50  \n",
      "  2400    |   0.443550   |   16.57  \n",
      "  2500    |   0.460629   |   16.58  \n",
      "  2600    |   0.473155   |   16.63  \n",
      "  2700    |   0.460911   |   16.60  \n",
      "  2800    |   0.426610   |   16.60  \n",
      "  2900    |   0.363549   |   16.59  \n",
      "  3000    |   0.528330   |   16.63  \n",
      "  3100    |   0.389533   |   16.62  \n",
      "  3200    |   0.478484   |   16.62  \n",
      "  3300    |   0.382110   |   16.68  \n",
      "  3400    |   0.497127   |   16.70  \n",
      "  3500    |   0.510514   |   16.59  \n",
      "  3600    |   0.466211   |   16.52  \n",
      "  3700    |   0.411301   |   16.59  \n",
      "  3800    |   0.401628   |   19.69  \n",
      "  3900    |   0.529289   |   22.19  \n",
      "  4000    |   0.457114   |   22.32  \n",
      "  4100    |   0.468055   |   20.92  \n",
      "  4200    |   0.463484   |   19.59  \n",
      "  4300    |   0.411947   |   16.75  \n",
      "  4400    |   0.505825   |   16.02  \n",
      "  4500    |   0.398549   |   16.03  \n",
      "  4600    |   0.406119   |   16.21  \n",
      "  4700    |   0.445389   |   16.01  \n",
      "  4800    |   0.422398   |   16.03  \n",
      "  4900    |   0.455475   |   16.02  \n",
      "  5000    |   0.362337   |   16.01  \n",
      "  5100    |   0.421687   |   16.03  \n",
      "  5200    |   0.441849   |   16.09  \n",
      "  5300    |   0.439083   |   16.03  \n",
      "  5400    |   0.415041   |   16.09  \n",
      "  5500    |   0.313565   |   15.99  \n",
      "  5600    |   0.456691   |   16.00  \n",
      "  5700    |   0.427029   |   16.07  \n",
      "  5737    |   0.418017   |   5.94   \n",
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.445992    |  0.520556  |       83.75       |  1041.12 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "roberta_train(roberta_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds, labels = get_predictions(roberta_classifier, val_dataloader)\n",
    "torch.save(roberta_classifier.state_dict(), \"models/trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
