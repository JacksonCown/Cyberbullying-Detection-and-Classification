{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661a9f63",
   "metadata": {},
   "source": [
    "This iPython notebook is dedicated to maintaining the cleaning pipeline for the text data. It contains tools and for processing each tweet as well as a function to generate the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to \n",
    "#!python -m spacy download en_core_web_md\n",
    "#!pip3 install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb3531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# NLTK Resources\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "# Init Tools\n",
    "nlp = spacy.load('en_core_web_md', disable =['ner', 'parser', 'textcat'])\n",
    "# This leaves #words and @words untouched\n",
    "tokenizer = RegexpTokenizer(r\"(@\\w+|#\\w+|\\w+)\") # r-string literal\n",
    "# Lemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb2e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: \n",
      "i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n"
     ]
    }
   ],
   "source": [
    "# Checking the NLTK Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stpWords = stopwords.words('english')\n",
    "print(\"Stop words: \")\n",
    "print(', '.join(stpWords))\n",
    "\n",
    "# List of Parts-of-Speech to remove\n",
    "stpPOS = ['d', 'x', '.', 'p', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bcb99",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b25a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the username's mentioned in a tweet\n",
    "def extract_usernames(text):\n",
    "    usernames = re.findall(r'@\\s*\\w+', text)\n",
    "    return \" \".join([user.strip('@') for user in usernames])\n",
    "\n",
    "# @user subbing function to apply to whole text column\n",
    "def sub_usernames(text):\n",
    "    cleaned = re.sub(r'@\\w+', '@username', text)\n",
    "    return cleaned\n",
    "    #return re.sub(r'@', '', cleaned)\n",
    "\n",
    "# Extracts the Hashtags and returns them as a single string\n",
    "def extract_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\s*\\w+', text)\n",
    "    return \" \".join([tag.strip('#') for tag in hashtags])\n",
    "\n",
    "# Replaces any Hashtag with a generic #hashtag\n",
    "def remove_hashtags(text):\n",
    "    #cleaned = re.sub(r'#\\w+', '#hashtag', text) # Temp Change from this for NB training\n",
    "    #cleaned = re.sub(r'#\\w+', '', text)\n",
    "    #return cleaned\n",
    "    return re.sub(r'#', '', text) # TEMP\n",
    "\n",
    "# Remove Punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    cleaned = tokenizer.tokenize(text)\n",
    "    return ' '.join(cleaned) # TEMP\n",
    "    #return cleaned\n",
    "\n",
    "# Helper function to prep the POS tagged words for lemmatizing\n",
    "# Code sampled from my NLTK Repo:\n",
    "# https://github.com/JacksonCown/Workshop-NLTK/blob/main/NLTK%20Short%20Examples-checkpoint.ipynb\n",
    "def pos_for_lem(words_pos):\n",
    "    newTags = []\n",
    "    for tup in words_pos:\n",
    "        converted = tup[1][0].lower() # Takes the first letter of the lowercase tag\n",
    "        newTup = (tup[0], converted)\n",
    "        newTags.append(newTup)\n",
    "    return newTags\n",
    "\n",
    "# Tag tweet parts-of-speech for lemmatizer\n",
    "# Input tweet must be tokenized list\n",
    "# Must be done before removing stop words\n",
    "def tag_pos(words):\n",
    "    words = nltk.pos_tag(words, tagset='universal') # universal tagset\n",
    "    # Process with helper fuction before returning\n",
    "    return pos_for_lem(words)\n",
    "\n",
    "# Remove Spacy Stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# This uses NLTK stop words, it will be much slower for now but it solves\n",
    "# an issues I am having where \"#hashtag\" --> \"# hashtag\"\n",
    "# Remove stop words after pos tagging\n",
    "def remove_stopwords_pos(text_pos):\n",
    "    cleaned_list = []\n",
    "    for word in text_pos:\n",
    "        word_lower = word[0].lower()\n",
    "        # This can be improved\n",
    "        if word_lower not in stpWords and word[1] not in stpPOS:\n",
    "            cleaned_list.append((word_lower, word[1]))\n",
    "    return cleaned_list\n",
    "\n",
    "# Lemmatize the part of speech tagged tweet\n",
    "def lemmatize_tweet(text_pos):\n",
    "    lemmatized = [lemm.lemmatize(word, pos) for word, pos in text_pos]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Detect Language of Tweet Text\n",
    "def detect_language(text):\n",
    "    # Attempt to Detect Tweet Language\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return \"lang_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae52c",
   "metadata": {},
   "source": [
    "#### Execution Order Per Tweet:\n",
    "\n",
    "1. Detect and Filter Language\n",
    "2. Clean Usernames\n",
    "3. Clean Hashtags\n",
    "4. Clean Punctuation\n",
    "5. Tag Parts-Of-Speech\n",
    "6. Clean Stop Words\n",
    "7. Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c61bd",
   "metadata": {},
   "source": [
    "#### Main Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f6ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column of labels for language detected\n",
    "def df_lang_detect(df):\n",
    "    df[\"lang\"] = df[\"tweet_text\"].apply(detect_language)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the dataframe tweet text\n",
    "# Also create a new column containing the extracted usernames\n",
    "def df_clean_usernames(df):\n",
    "    df[\"mentioned_users\"] = df[\"tweet_text\"].apply(extract_usernames)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(sub_usernames)\n",
    "    return df\n",
    "\n",
    "# Remove all hashtags from the dataframe tweet text\n",
    "# Also create a new column containing the extracted hashtags\n",
    "def df_clean_hashtags(df):\n",
    "    df[\"hashtags\"] = df[\"tweet_text\"].apply(extract_hashtags)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_hashtags)\n",
    "    return df\n",
    "\n",
    "# Remove all punctuation from the data frame tweet text\n",
    "def df_clean_punctuation(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_punctuation)\n",
    "    return df\n",
    "\n",
    "# Tag all Parts-of-Speech in the tweet column\n",
    "def df_pos_tag(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(tag_pos)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the data frame tweet text.\n",
    "# Much slower using remove_stopwords_pos than remove_stopwords\n",
    "# while I am trying to fix issue with hashtag tokenization\n",
    "def df_clean_stopwords(df):\n",
    "    #df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_stopwords_pos)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_stopwords)\n",
    "    return df\n",
    "\n",
    "# Lemmatize the tweet text column of the dataframe\n",
    "# Tag parts of speech and remove stopwords first\n",
    "def df_lemmatize(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lemmatize_tweet)\n",
    "    return df\n",
    "\n",
    "# Entire Cleaning Pipeline\n",
    "def df_clean(df):\n",
    "    print(\"Cleaning DataFrame: Starting\\n\")\n",
    "    df = df_lang_detect(df)\n",
    "    print(\"Language Filtering: Complete\")\n",
    "    df = df_clean_usernames(df)\n",
    "    print(\"Substitute Usernames: Complete\")\n",
    "    df = df_clean_hashtags(df)\n",
    "    print(\"Clean Hashtags: Complete\")\n",
    "    df = df_clean_punctuation(df)\n",
    "    print(\"Clean Punctuation: Complete\")\n",
    "    #df = df_pos_tag(df)\n",
    "    #print(\"Tag Parts-of-Speech: Complete\")\n",
    "    df = df_clean_stopwords(df)\n",
    "    print(\"Remove Stop Words: Complete\")\n",
    "    #df = df_lemmatize(df)\n",
    "    #print(\"Lemmatize Text: Complete\")\n",
    "    # Filter To English Samples\n",
    "    return df[df[\"lang\"] == \"en\"]\n",
    "\n",
    "# Function to call the cleaning pipeline and write the\n",
    "# final dataset to memory\n",
    "def pipeline(path=\"data/cleaned/cleaned_lemmatized_english.csv\"):\n",
    "    df = pd.read_csv(\"data/cyberbullying_tweets.csv\")\n",
    "    df = df_clean(df)\n",
    "    df.to_csv(path)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6e8b4",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0558eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: Starting\n",
      "\n",
      "Language Filtering: Complete\n",
      "Substitute Usernames: Complete\n",
      "Clean Hashtags: Complete\n",
      "Clean Punctuation: Complete\n",
      "Remove Stop Words: Complete\n",
      "data/cleaned/nohashtag_cleaned_unlemmatized_english.csv\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to generate entire dataset\n",
    "#pipeline\n",
    "pipeline(\"data/cleaned/nohashtag_cleaned_lemmatized_english.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075af2ed",
   "metadata": {},
   "source": [
    "### Testing on 30 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a6064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Raja5aab @Quickieleaks Yes, the test of god i...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Itu sekolah ya bukan tempat bully! Ga jauh kay...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karma. I hope it bites Kat on the butt. She is...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@stockputout everything but mostly my priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rebecca Black Drops Out of School Due to Bully...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Jord_Is_Dead http://t.co/UsQInYW5Gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Bully flushes on KD http://twitvid.com/A2TNP</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ughhhh #MKR</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @Kurdsnews: Turkish state has killed 241 ch...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Love that the best response to the hotcakes th...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@sarinhacoral @Victor_Maggi tadinhu de mim , s...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@0xabad1dea @kelseytheodore2 twitter is basica...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Best pick up line? Hi, you're cute... ?: I lov...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Now I gotta walk to classss?! I officially hat...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@halalcunty @biebervalue @liamxkiwi @greenline...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I still have Jack, Amsterdam, Ciroc, Crown, Bu...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@scottyswaggod men are the ones that are going...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wishing my arena partner was on. &amp;gt;.&amp;gt;  Re...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Nation: Bullying | Turn to 10 http:/...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@sand_dejesus Isso é bullying! @O_Patriarca</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@gcarothers eek. i can't stand split keyboards...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@MaxBlumenthal @cpassevant @anadumitrescu13 Po...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know there are people out there who like @...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type\n",
       "0   In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1   Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2   @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3   @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4   @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "5   @Raja5aab @Quickieleaks Yes, the test of god i...  not_cyberbullying\n",
       "6   Itu sekolah ya bukan tempat bully! Ga jauh kay...  not_cyberbullying\n",
       "7   Karma. I hope it bites Kat on the butt. She is...  not_cyberbullying\n",
       "8        @stockputout everything but mostly my priest  not_cyberbullying\n",
       "9   Rebecca Black Drops Out of School Due to Bully...  not_cyberbullying\n",
       "10               @Jord_Is_Dead http://t.co/UsQInYW5Gn  not_cyberbullying\n",
       "11   The Bully flushes on KD http://twitvid.com/A2TNP  not_cyberbullying\n",
       "12                                        Ughhhh #MKR  not_cyberbullying\n",
       "13  RT @Kurdsnews: Turkish state has killed 241 ch...  not_cyberbullying\n",
       "14  Love that the best response to the hotcakes th...  not_cyberbullying\n",
       "15  @yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...  not_cyberbullying\n",
       "16  @sarinhacoral @Victor_Maggi tadinhu de mim , s...  not_cyberbullying\n",
       "17  @0xabad1dea @kelseytheodore2 twitter is basica...  not_cyberbullying\n",
       "18  Best pick up line? Hi, you're cute... ?: I lov...  not_cyberbullying\n",
       "19  Now I gotta walk to classss?! I officially hat...  not_cyberbullying\n",
       "20  @halalcunty @biebervalue @liamxkiwi @greenline...  not_cyberbullying\n",
       "21  Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...  not_cyberbullying\n",
       "22  I still have Jack, Amsterdam, Ciroc, Crown, Bu...  not_cyberbullying\n",
       "23  @scottyswaggod men are the ones that are going...  not_cyberbullying\n",
       "24  Wishing my arena partner was on. &gt;.&gt;  Re...  not_cyberbullying\n",
       "25  Education Nation: Bullying | Turn to 10 http:/...  not_cyberbullying\n",
       "26        @sand_dejesus Isso é bullying! @O_Patriarca  not_cyberbullying\n",
       "27  @gcarothers eek. i can't stand split keyboards...  not_cyberbullying\n",
       "28  @MaxBlumenthal @cpassevant @anadumitrescu13 Po...  not_cyberbullying\n",
       "29  You know there are people out there who like @...  not_cyberbullying"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncleaned Dataset\n",
    "# Testing Entire Process on 30 samples\n",
    "df = pd.read_csv(\"data/cyberbullying_tweets.csv\").iloc[0:30]\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7cdbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: Starting\n",
      "\n",
      "Language Filtering: Complete\n",
      "Substitute Usernames: Complete\n",
      "Clean Hashtags: Complete\n",
      "Clean Punctuation: Complete\n",
      "Remove Stop Words: Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>words katandandre food crapilicious mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>katandandre mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aussietv white MKR theblock ImACelebrityAU tod...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>aussietv MKR theblock ImACelebrityAU today sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@username classy whore red velvet cupcakes</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>XochitlSuckkks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@username meh P thanks heads concerned angry d...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Jason_Gio</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@username ISIS account pretending Kurdish acco...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>RudhoeEnglish</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@username @username Yes test god good bad indi...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Raja5aab Quickieleaks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karma hope bites Kat butt nasty mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@username priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>stockputout</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rebecca Black Drops School Bullying</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@username http t co UsQInYW5Gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Jord_Is_Dead</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bully flushes KD http twitvid com A2TNP</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @username Turkish state killed 241 children...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Kurdsnews</td>\n",
       "      <td>news GoogleÇeviriciTopluluğuKürtçeyideE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Love best response hotcakes managed film non c...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@username @username twitter basically angry le...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>0xabad1dea kelseytheodore2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Best pick line Hi cute love people James Potte...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>got ta walk classss officially hate stupid bus...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@username @username @username @username @usern...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>halalcunty biebervalue liamxkiwi greenlinerzjm...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Jack Amsterdam Ciroc Crown Bud Light Lime rita...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@username men ones going push real change ones...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>scottyswaggod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wishing arena partner gt gt want PvP happening</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Nation Bullying Turn 10 http t co sx...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@username Isso é bullying @username</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>sand_dejesus O_Patriarca</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@username eek t stand split keyboards doesn t ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>gcarothers</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@username @username @username Post Hebdo LOL E...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>MaxBlumenthal cpassevant anadumitrescu13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>know people like @username don t listen old sc...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>joeyBADASS_</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type lang  \\\n",
       "0             words katandandre food crapilicious mkr  not_cyberbullying   en   \n",
       "1   aussietv white MKR theblock ImACelebrityAU tod...  not_cyberbullying   en   \n",
       "2          @username classy whore red velvet cupcakes  not_cyberbullying   en   \n",
       "3   @username meh P thanks heads concerned angry d...  not_cyberbullying   en   \n",
       "4   @username ISIS account pretending Kurdish acco...  not_cyberbullying   en   \n",
       "5   @username @username Yes test god good bad indi...  not_cyberbullying   en   \n",
       "7                 Karma hope bites Kat butt nasty mkr  not_cyberbullying   en   \n",
       "8                                    @username priest  not_cyberbullying   en   \n",
       "9                 Rebecca Black Drops School Bullying  not_cyberbullying   en   \n",
       "10                     @username http t co UsQInYW5Gn  not_cyberbullying   en   \n",
       "11            Bully flushes KD http twitvid com A2TNP  not_cyberbullying   en   \n",
       "13  RT @username Turkish state killed 241 children...  not_cyberbullying   en   \n",
       "14  Love best response hotcakes managed film non c...  not_cyberbullying   en   \n",
       "17  @username @username twitter basically angry le...  not_cyberbullying   en   \n",
       "18  Best pick line Hi cute love people James Potte...  not_cyberbullying   en   \n",
       "19  got ta walk classss officially hate stupid bus...  not_cyberbullying   en   \n",
       "20  @username @username @username @username @usern...  not_cyberbullying   en   \n",
       "22  Jack Amsterdam Ciroc Crown Bud Light Lime rita...  not_cyberbullying   en   \n",
       "23  @username men ones going push real change ones...  not_cyberbullying   en   \n",
       "24     Wishing arena partner gt gt want PvP happening  not_cyberbullying   en   \n",
       "25  Education Nation Bullying Turn 10 http t co sx...  not_cyberbullying   en   \n",
       "26                @username Isso é bullying @username  not_cyberbullying   en   \n",
       "27  @username eek t stand split keyboards doesn t ...  not_cyberbullying   en   \n",
       "28  @username @username @username Post Hebdo LOL E...  not_cyberbullying   en   \n",
       "29  know people like @username don t listen old sc...  not_cyberbullying   en   \n",
       "\n",
       "                                      mentioned_users  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                      XochitlSuckkks   \n",
       "3                                           Jason_Gio   \n",
       "4                                       RudhoeEnglish   \n",
       "5                               Raja5aab Quickieleaks   \n",
       "7                                                       \n",
       "8                                         stockputout   \n",
       "9                                                       \n",
       "10                                       Jord_Is_Dead   \n",
       "11                                                      \n",
       "13                                          Kurdsnews   \n",
       "14                                                      \n",
       "17                         0xabad1dea kelseytheodore2   \n",
       "18                                                      \n",
       "19                                                      \n",
       "20  halalcunty biebervalue liamxkiwi greenlinerzjm...   \n",
       "22                                                      \n",
       "23                                      scottyswaggod   \n",
       "24                                                      \n",
       "25                                                      \n",
       "26                           sand_dejesus O_Patriarca   \n",
       "27                                         gcarothers   \n",
       "28           MaxBlumenthal cpassevant anadumitrescu13   \n",
       "29                                        joeyBADASS_   \n",
       "\n",
       "                                             hashtags  \n",
       "0                                     katandandre mkr  \n",
       "1   aussietv MKR theblock ImACelebrityAU today sun...  \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "7                                                 mkr  \n",
       "8                                                      \n",
       "9                                                      \n",
       "10                                                     \n",
       "11                                                     \n",
       "13            news GoogleÇeviriciTopluluğuKürtçeyideE  \n",
       "14                                                MKR  \n",
       "17                                                     \n",
       "18                                                     \n",
       "19                                                     \n",
       "20                                                     \n",
       "22                                                     \n",
       "23                                                     \n",
       "24                                                     \n",
       "25                                                     \n",
       "26                                                     \n",
       "27                                                     \n",
       "28                                                     \n",
       "29                                                     "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sample clean\n",
    "df = df_clean(df)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679939ca",
   "metadata": {},
   "source": [
    "### Testing Functions for individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5be4c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Tweet:  Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Sub Usernames X1:  Bruh what bruh @username #bruh: of to did  the #ruh - #hello # @username. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Hashtags X2:  Bruh what bruh @username #hashtag: of to did  the #hashtag - #hashtag # @username. I am is wowow, #hashtag, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Punctuation X3:  ['Bruh', 'what', 'bruh', '@username', '#hashtag', 'of', 'to', 'did', 'the', '#hashtag', '#hashtag', '@username', 'I', 'am', 'is', 'wowow', '#hashtag', 'a', 'videos', 'wtf'] \n",
      "\n",
      "Tag Parts-of-Speech X4:  [('Bruh', 'n'), ('what', 'p'), ('bruh', 'v'), ('@username', 'a'), ('#hashtag', 'n'), ('of', 'a'), ('to', 'p'), ('did', 'v'), ('the', 'd'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('@username', 'n'), ('I', 'p'), ('am', 'v'), ('is', 'v'), ('wowow', 'a'), ('#hashtag', 'a'), ('a', 'd'), ('videos', 'n'), ('wtf', 'n')] \n",
      "\n",
      "Remove Stop Words X5:  [('bruh', 'n'), ('bruh', 'v'), ('@username', 'a'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('@username', 'n'), ('wowow', 'a'), ('#hashtag', 'a'), ('videos', 'n'), ('wtf', 'n')] \n",
      "\n",
      "Lemmatize Tweet X6:  bruh bruh @username #hashtag #hashtag #hashtag @username wowow #hashtag video wtf \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing For Singular Tweet Cleaning Fuctions\n",
    "# Very messy sample tweet to clean\n",
    "fake_tweet = \"Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]]\"\n",
    "# Very simple pipeline tester\n",
    "def test_pipeline(tweet=fake_tweet):\n",
    "    print(\"Source Tweet: \", tweet, \"\\n\")\n",
    "    x1 = sub_usernames(fake_tweet)\n",
    "    print(\"Sub Usernames X1: \",x1, \"\\n\")\n",
    "    x2 = remove_hashtags(x1)\n",
    "    print(\"Remove Hashtags X2: \",x2, \"\\n\")\n",
    "    x3 = remove_punctuation(x2)\n",
    "    print(\"Remove Punctuation X3: \",x3, \"\\n\")\n",
    "    x4 = tag_pos(x3) # universal tagset\n",
    "    print(\"Tag Parts-of-Speech X4: \",x4, \"\\n\")\n",
    "    x5 = remove_stopwords_pos(x4)\n",
    "    print(\"Remove Stop Words X5: \", x5, \"\\n\")\n",
    "    x6 = lemmatize_tweet(x5)\n",
    "    print(\"Lemmatize Tweet X6: \", x6, \"\\n\")\n",
    "\n",
    "test_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
