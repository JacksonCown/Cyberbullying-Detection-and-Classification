{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661a9f63",
   "metadata": {},
   "source": [
    "This iPython notebook is dedicated to maintaining the cleaning pipeline for the text data. It contains tools and for processing each tweet as well as a function to generate the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to \n",
    "#!python -m spacy download en_core_web_md\n",
    "#!pip3 install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb3531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# NLTK Resources\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "# Init Tools\n",
    "nlp = spacy.load('en_core_web_md', disable =['ner', 'parser', 'textcat'])\n",
    "# This leaves #words and @words untouched\n",
    "tokenizer = RegexpTokenizer(r\"(@\\w+|#\\w+|\\w+)\") # r-string literal\n",
    "# Lemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb2e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: \n",
      "i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n"
     ]
    }
   ],
   "source": [
    "# Checking the NLTK Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stpWords = stopwords.words('english')\n",
    "print(\"Stop words: \")\n",
    "print(', '.join(stpWords))\n",
    "\n",
    "# List of Parts-of-Speech to remove\n",
    "stpPOS = ['d', 'x', '.', 'p', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bcb99",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b25a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the username's mentioned in a tweet\n",
    "def extract_usernames(text):\n",
    "    usernames = re.findall(r'@\\s*\\w+', text)\n",
    "    return \" \".join([user.strip('@') for user in usernames])\n",
    "\n",
    "# @user subbing function to apply to whole text column\n",
    "def sub_usernames(text):\n",
    "    cleaned = re.sub(r'@\\w+', '@username', text)\n",
    "    return cleaned\n",
    "\n",
    "# Extracts the Hashtags and returns them as a single string\n",
    "def extract_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\s*\\w+', text)\n",
    "    return \" \".join([tag.strip('#') for tag in hashtags])\n",
    "\n",
    "# Replaces any Hashtag with a generic #hashtag\n",
    "def remove_hashtags(text):\n",
    "    cleaned = re.sub(r'#\\w+', '#hashtag', text)\n",
    "    return cleaned\n",
    "\n",
    "# Remove Punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    cleaned = tokenizer.tokenize(text)\n",
    "    #return ' '.join(cleaned) # TEMP\n",
    "    return cleaned\n",
    "\n",
    "# Helper function to prep the POS tagged words for lemmatizing\n",
    "# Code sampled from my NLTK Repo:\n",
    "# https://github.com/JacksonCown/Workshop-NLTK/blob/main/NLTK%20Short%20Examples-checkpoint.ipynb\n",
    "def pos_for_lem(words_pos):\n",
    "    newTags = []\n",
    "    for tup in words_pos:\n",
    "        converted = tup[1][0].lower() # Takes the first letter of the lowercase tag\n",
    "        newTup = (tup[0], converted)\n",
    "        newTags.append(newTup)\n",
    "    return newTags\n",
    "\n",
    "# Tag tweet parts-of-speech for lemmatizer\n",
    "# Input tweet must be tokenized list\n",
    "# Must be done before removing stop words\n",
    "def tag_pos(words):\n",
    "    words = nltk.pos_tag(words, tagset='universal') # universal tagset\n",
    "    # Process with helper fuction before returning\n",
    "    return pos_for_lem(words)\n",
    "\n",
    "# Remove Spacy Stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# This uses NLTK stop words, it will be much slower for now but it solves\n",
    "# an issues I am having where \"#hashtag\" --> \"# hashtag\"\n",
    "# Remove stop words after pos tagging\n",
    "def remove_stopwords_pos(text_pos):\n",
    "    cleaned_list = []\n",
    "    for word in text_pos:\n",
    "        word_lower = word[0].lower()\n",
    "        # This can be improved\n",
    "        if word_lower not in stpWords and word[1] not in stpPOS:\n",
    "            cleaned_list.append((word_lower, word[1]))\n",
    "    return cleaned_list\n",
    "\n",
    "# Lemmatize the part of speech tagged tweet\n",
    "def lemmatize_tweet(text_pos):\n",
    "    lemmatized = [lemm.lemmatize(word, pos) for word, pos in text_pos]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Detect Language of Tweet Text\n",
    "def detect_language(text):\n",
    "    # Attempt to Detect Tweet Language\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return \"lang_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae52c",
   "metadata": {},
   "source": [
    "#### Execution Order Per Tweet:\n",
    "\n",
    "1. Detect and Filter Language\n",
    "2. Clean Usernames\n",
    "3. Clean Hashtags\n",
    "4. Clean Punctuation\n",
    "5. Tag Parts-Of-Speech\n",
    "6. Clean Stop Words\n",
    "7. Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c61bd",
   "metadata": {},
   "source": [
    "#### Main Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f6ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column of labels for language detected\n",
    "def df_lang_detect(df):\n",
    "    df[\"lang\"] = df[\"tweet_text\"].apply(detect_language)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the dataframe tweet text\n",
    "# Also create a new column containing the extracted usernames\n",
    "def df_clean_usernames(df):\n",
    "    df[\"mentioned_users\"] = df[\"tweet_text\"].apply(extract_usernames)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(sub_usernames)\n",
    "    return df\n",
    "\n",
    "# Remove all hashtags from the dataframe tweet text\n",
    "# Also create a new column containing the extracted hashtags\n",
    "def df_clean_hashtags(df):\n",
    "    df[\"hashtags\"] = df[\"tweet_text\"].apply(extract_hashtags)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_hashtags)\n",
    "    return df\n",
    "\n",
    "# Remove all punctuation from the data frame tweet text\n",
    "def df_clean_punctuation(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_punctuation)\n",
    "    return df\n",
    "\n",
    "# Tag all Parts-of-Speech in the tweet column\n",
    "def df_pos_tag(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(tag_pos)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the data frame tweet text.\n",
    "# Much slower using remove_stopwords_pos than remove_stopwords\n",
    "# while I am trying to fix issue with hashtag tokenization\n",
    "def df_clean_stopwords(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_stopwords_pos)\n",
    "    return df\n",
    "\n",
    "# Lemmatize the tweet text column of the dataframe\n",
    "# Tag parts of speech and remove stopwords first\n",
    "def df_lemmatize(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lemmatize_tweet)\n",
    "    return df\n",
    "\n",
    "# Entire Cleaning Pipeline\n",
    "def df_clean(df):\n",
    "    print(\"Cleaning DataFrame: Starting\\n\")\n",
    "    df = df_lang_detect(df)\n",
    "    print(\"Language Filtering: Complete\")\n",
    "    df = df_clean_usernames(df)\n",
    "    print(\"Substitute Usernames: Complete\")\n",
    "    df = df_clean_hashtags(df)\n",
    "    print(\"Clean Hashtags: Complete\")\n",
    "    df = df_clean_punctuation(df)\n",
    "    print(\"Clean Punctuation: Complete\")\n",
    "    df = df_pos_tag(df)\n",
    "    print(\"Tag Parts-of-Speech: Complete\")\n",
    "    df = df_clean_stopwords(df)\n",
    "    print(\"Remove Stop Words: Complete\")\n",
    "    df = df_lemmatize(df)\n",
    "    print(\"Lemmatize Text: Complete\")\n",
    "    # Filter To English Samples\n",
    "    return df[df[\"lang\"] == \"en\"]\n",
    "\n",
    "# Function to call the cleaning pipeline and write the\n",
    "# final dataset to memory\n",
    "def pipeline(path=\"data/cleaned/cleaned_lemmatized_english.csv\"):\n",
    "    df = pd.read_csv(\"data/cyberbullying_tweets.csv\")\n",
    "    df = df_clean(df)\n",
    "    df.to_csv(path)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6e8b4",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0558eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate entire dataset\n",
    "#pipeline\n",
    "#pipeline(\"data/cleaned/nohashtag_cleaned_lemmatized_english.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075af2ed",
   "metadata": {},
   "source": [
    "### Testing on 30 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a6064e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Raja5aab @Quickieleaks Yes, the test of god i...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Itu sekolah ya bukan tempat bully! Ga jauh kay...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karma. I hope it bites Kat on the butt. She is...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@stockputout everything but mostly my priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rebecca Black Drops Out of School Due to Bully...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Jord_Is_Dead http://t.co/UsQInYW5Gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Bully flushes on KD http://twitvid.com/A2TNP</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ughhhh #MKR</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @Kurdsnews: Turkish state has killed 241 ch...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Love that the best response to the hotcakes th...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@sarinhacoral @Victor_Maggi tadinhu de mim , s...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@0xabad1dea @kelseytheodore2 twitter is basica...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Best pick up line? Hi, you're cute... ?: I lov...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Now I gotta walk to classss?! I officially hat...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@halalcunty @biebervalue @liamxkiwi @greenline...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I still have Jack, Amsterdam, Ciroc, Crown, Bu...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@scottyswaggod men are the ones that are going...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wishing my arena partner was on. &amp;gt;.&amp;gt;  Re...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Nation: Bullying | Turn to 10 http:/...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@sand_dejesus Isso é bullying! @O_Patriarca</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@gcarothers eek. i can't stand split keyboards...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@MaxBlumenthal @cpassevant @anadumitrescu13 Po...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know there are people out there who like @...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type\n",
       "0   In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1   Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2   @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3   @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4   @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "5   @Raja5aab @Quickieleaks Yes, the test of god i...  not_cyberbullying\n",
       "6   Itu sekolah ya bukan tempat bully! Ga jauh kay...  not_cyberbullying\n",
       "7   Karma. I hope it bites Kat on the butt. She is...  not_cyberbullying\n",
       "8        @stockputout everything but mostly my priest  not_cyberbullying\n",
       "9   Rebecca Black Drops Out of School Due to Bully...  not_cyberbullying\n",
       "10               @Jord_Is_Dead http://t.co/UsQInYW5Gn  not_cyberbullying\n",
       "11   The Bully flushes on KD http://twitvid.com/A2TNP  not_cyberbullying\n",
       "12                                        Ughhhh #MKR  not_cyberbullying\n",
       "13  RT @Kurdsnews: Turkish state has killed 241 ch...  not_cyberbullying\n",
       "14  Love that the best response to the hotcakes th...  not_cyberbullying\n",
       "15  @yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...  not_cyberbullying\n",
       "16  @sarinhacoral @Victor_Maggi tadinhu de mim , s...  not_cyberbullying\n",
       "17  @0xabad1dea @kelseytheodore2 twitter is basica...  not_cyberbullying\n",
       "18  Best pick up line? Hi, you're cute... ?: I lov...  not_cyberbullying\n",
       "19  Now I gotta walk to classss?! I officially hat...  not_cyberbullying\n",
       "20  @halalcunty @biebervalue @liamxkiwi @greenline...  not_cyberbullying\n",
       "21  Kids Love😘❤ @ Mohamad Bin Zayed City مدينة محم...  not_cyberbullying\n",
       "22  I still have Jack, Amsterdam, Ciroc, Crown, Bu...  not_cyberbullying\n",
       "23  @scottyswaggod men are the ones that are going...  not_cyberbullying\n",
       "24  Wishing my arena partner was on. &gt;.&gt;  Re...  not_cyberbullying\n",
       "25  Education Nation: Bullying | Turn to 10 http:/...  not_cyberbullying\n",
       "26        @sand_dejesus Isso é bullying! @O_Patriarca  not_cyberbullying\n",
       "27  @gcarothers eek. i can't stand split keyboards...  not_cyberbullying\n",
       "28  @MaxBlumenthal @cpassevant @anadumitrescu13 Po...  not_cyberbullying\n",
       "29  You know there are people out there who like @...  not_cyberbullying"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncleaned Dataset\n",
    "# Testing Entire Process on 30 samples\n",
    "df = pd.read_csv(\"data/cyberbullying_tweets.csv\").iloc[0:30]\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7cdbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: Starting\n",
      "\n",
      "Language Filtering: Complete\n",
      "Substitute Usernames: Complete\n",
      "Clean Hashtags: Complete\n",
      "Clean Punctuation: Complete\n",
      "Tag Parts-of-Speech: Complete\n",
      "Remove Stop Words: Complete\n",
      "Lemmatize Text: Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word #hashtag food crapilicious #hashtag</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>katandandre mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#hashtag white #hashtag #hashtag #hashtag #has...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>aussietv MKR theblock ImACelebrityAU today sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@username classy whore red velvet cupcake</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>XochitlSuckkks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@username meh p thanks head concern angry dude...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Jason_Gio</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@username isi account pretend kurdish account ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>RudhoeEnglish</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@username @username yes test god good bad indi...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Raja5aab Quickieleaks</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>karma hope bite kat butt nasty #hashtag</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@username everything mostly priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>stockputout</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rebecca black drop school due bullying</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@username http co usqinyw5gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Jord_Is_Dead</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bully flush kd http twitvid com a2tnp</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rt @username turkish state kill 241 child last...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>Kurdsnews</td>\n",
       "      <td>news GoogleÇeviriciTopluluğuKürtçeyideE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>love best response hotcake manage film non com...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@username @username twitter basically angry le...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>0xabad1dea kelseytheodore2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>best pick line hi cute love people call james ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gotta walk class officially hate stupid bus sy...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@username @username @username @username @usern...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>halalcunty biebervalue liamxkiwi greenlinerzjm...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>still jack amsterdam ciroc crown bud light lim...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@username men one go push real change one powe...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>scottyswaggod</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>wish arena partner gt gt really want get pvp h...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>education nation bullying turn 10 http co sxtiwtp</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@username eek stand split keyboard work well mmos</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>gcarothers</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@username @username @username post hebdo lol e...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>MaxBlumenthal cpassevant anadumitrescu13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>know people like @username listen old school</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>joeyBADASS_</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type lang  \\\n",
       "0            word #hashtag food crapilicious #hashtag  not_cyberbullying   en   \n",
       "1   #hashtag white #hashtag #hashtag #hashtag #has...  not_cyberbullying   en   \n",
       "2           @username classy whore red velvet cupcake  not_cyberbullying   en   \n",
       "3   @username meh p thanks head concern angry dude...  not_cyberbullying   en   \n",
       "4   @username isi account pretend kurdish account ...  not_cyberbullying   en   \n",
       "5   @username @username yes test god good bad indi...  not_cyberbullying   en   \n",
       "7             karma hope bite kat butt nasty #hashtag  not_cyberbullying   en   \n",
       "8                  @username everything mostly priest  not_cyberbullying   en   \n",
       "9              rebecca black drop school due bullying  not_cyberbullying   en   \n",
       "10                       @username http co usqinyw5gn  not_cyberbullying   en   \n",
       "11              bully flush kd http twitvid com a2tnp  not_cyberbullying   en   \n",
       "13  rt @username turkish state kill 241 child last...  not_cyberbullying   en   \n",
       "14  love best response hotcake manage film non com...  not_cyberbullying   en   \n",
       "17  @username @username twitter basically angry le...  not_cyberbullying   en   \n",
       "18  best pick line hi cute love people call james ...  not_cyberbullying   en   \n",
       "19  gotta walk class officially hate stupid bus sy...  not_cyberbullying   en   \n",
       "20  @username @username @username @username @usern...  not_cyberbullying   en   \n",
       "22  still jack amsterdam ciroc crown bud light lim...  not_cyberbullying   en   \n",
       "23  @username men one go push real change one powe...  not_cyberbullying   en   \n",
       "24  wish arena partner gt gt really want get pvp h...  not_cyberbullying   en   \n",
       "25  education nation bullying turn 10 http co sxtiwtp  not_cyberbullying   en   \n",
       "27  @username eek stand split keyboard work well mmos  not_cyberbullying   en   \n",
       "28  @username @username @username post hebdo lol e...  not_cyberbullying   en   \n",
       "29       know people like @username listen old school  not_cyberbullying   en   \n",
       "\n",
       "                                      mentioned_users  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                      XochitlSuckkks   \n",
       "3                                           Jason_Gio   \n",
       "4                                       RudhoeEnglish   \n",
       "5                               Raja5aab Quickieleaks   \n",
       "7                                                       \n",
       "8                                         stockputout   \n",
       "9                                                       \n",
       "10                                       Jord_Is_Dead   \n",
       "11                                                      \n",
       "13                                          Kurdsnews   \n",
       "14                                                      \n",
       "17                         0xabad1dea kelseytheodore2   \n",
       "18                                                      \n",
       "19                                                      \n",
       "20  halalcunty biebervalue liamxkiwi greenlinerzjm...   \n",
       "22                                                      \n",
       "23                                      scottyswaggod   \n",
       "24                                                      \n",
       "25                                                      \n",
       "27                                         gcarothers   \n",
       "28           MaxBlumenthal cpassevant anadumitrescu13   \n",
       "29                                        joeyBADASS_   \n",
       "\n",
       "                                             hashtags  \n",
       "0                                     katandandre mkr  \n",
       "1   aussietv MKR theblock ImACelebrityAU today sun...  \n",
       "2                                                      \n",
       "3                                                      \n",
       "4                                                      \n",
       "5                                                      \n",
       "7                                                 mkr  \n",
       "8                                                      \n",
       "9                                                      \n",
       "10                                                     \n",
       "11                                                     \n",
       "13            news GoogleÇeviriciTopluluğuKürtçeyideE  \n",
       "14                                                MKR  \n",
       "17                                                     \n",
       "18                                                     \n",
       "19                                                     \n",
       "20                                                     \n",
       "22                                                     \n",
       "23                                                     \n",
       "24                                                     \n",
       "25                                                     \n",
       "27                                                     \n",
       "28                                                     \n",
       "29                                                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sample clean\n",
    "df = df_clean(df)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679939ca",
   "metadata": {},
   "source": [
    "### Testing Functions for individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5be4c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Tweet:  Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Sub Usernames X1:  Bruh what bruh @username #bruh: of to did  the #ruh - #hello # @username. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Hashtags X2:  Bruh what bruh @username bruh: of to did  the ruh - hello  @username. I am is wowow, gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Punctuation X3:  Bruh what bruh @username bruh of to did the ruh hello @username I am is wowow gaming a videos wtf \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     x6 \u001b[38;5;241m=\u001b[39m lemmatize_tweet(x5)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatize Tweet X6: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x6, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtest_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtest_pipeline\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m     11\u001b[0m x3 \u001b[38;5;241m=\u001b[39m remove_punctuation(x2)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemove Punctuation X3: \u001b[39m\u001b[38;5;124m\"\u001b[39m,x3, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[43mtag_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# universal tagset\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTag Parts-of-Speech X4: \u001b[39m\u001b[38;5;124m\"\u001b[39m,x4, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m x5 \u001b[38;5;241m=\u001b[39m remove_stopwords_pos(x4)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mtag_pos\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtag_pos\u001b[39m(words):\n\u001b[1;32m---> 45\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muniversal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# universal tagset\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Process with helper fuction before returning\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pos_for_lem(words)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "# Testing For Singular Tweet Cleaning Fuctions\n",
    "# Very messy sample tweet to clean\n",
    "fake_tweet = \"Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]]\"\n",
    "# Very simple pipeline tester\n",
    "def test_pipeline(tweet=fake_tweet):\n",
    "    print(\"Source Tweet: \", tweet, \"\\n\")\n",
    "    x1 = sub_usernames(fake_tweet)\n",
    "    print(\"Sub Usernames X1: \",x1, \"\\n\")\n",
    "    x2 = remove_hashtags(x1)\n",
    "    print(\"Remove Hashtags X2: \",x2, \"\\n\")\n",
    "    x3 = remove_punctuation(x2)\n",
    "    print(\"Remove Punctuation X3: \",x3, \"\\n\")\n",
    "    x4 = tag_pos(x3) # universal tagset\n",
    "    print(\"Tag Parts-of-Speech X4: \",x4, \"\\n\")\n",
    "    x5 = remove_stopwords_pos(x4)\n",
    "    print(\"Remove Stop Words X5: \", x5, \"\\n\")\n",
    "    x6 = lemmatize_tweet(x5)\n",
    "    print(\"Lemmatize Tweet X6: \", x6, \"\\n\")\n",
    "\n",
    "test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ead5669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40362</th>\n",
       "      <td>@Goree_JuhssGuns hahaha he ain't even worth my...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15019</th>\n",
       "      <td>RT @hsaymssik: Sucks to have the smile wiped o...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46321</th>\n",
       "      <td>Just a reminder, it's absolutely disgusting to...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23927</th>\n",
       "      <td>RT @BuzzFeedUK: When you accidentally open you...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>Loving the look of the fritters! #mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46681</th>\n",
       "      <td>Has 2 interesting events last year involving r...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40139</th>\n",
       "      <td>NIGGERS is the real way you dumb fuck</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12668</th>\n",
       "      <td>things that AREN'T jokes - rape - sexism - rac...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36459</th>\n",
       "      <td>You were bullied? If so, I’m sorry that happen...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8442</th>\n",
       "      <td>Haha did you watch big brother?, “Zankie” was ...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12216</th>\n",
       "      <td>RT @JusFappinAround: @nflis that a female refe...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13293</th>\n",
       "      <td>men when they think saying any slur making rac...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27948</th>\n",
       "      <td>@GCMayhem Sorry, I don't converse with people ...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>&amp;#128514;&amp;#128514;&amp;#128514;&amp;#128514;&amp;#128514;&amp;...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21391</th>\n",
       "      <td>So you are saying any Muslim scholar who say e...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>I hear you on that! When you’re a teenager. Yo...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14068</th>\n",
       "      <td>Long ago, I asked some white gay men on our lu...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22587</th>\n",
       "      <td>@Abu_Baraa1 @Myself_00001 Now if we can only g...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42812</th>\n",
       "      <td>“@applecheeksxo: Wait what RT @tayyoung_: FUCK...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30506</th>\n",
       "      <td>sorry i'm not humble, dick.</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33581</th>\n",
       "      <td>if i went to school with draco idc if he was t...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17394</th>\n",
       "      <td>@discerningmumin That's idiotic. Not holding a...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45731</th>\n",
       "      <td>DEANGELO WHY ARE YOU SUCH A SORRY SACK OF DUMB...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828</th>\n",
       "      <td>@cervixseasoning Just a joke. I thought gay ra...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25315</th>\n",
       "      <td>A lot of indie game devs remind me a LOT of FO...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15821</th>\n",
       "      <td>The missus loves #MKR if I have to watch five ...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45951</th>\n",
       "      <td>RT @boycottdbanj: @ChyyChy fuck you coon.WE CA...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24379</th>\n",
       "      <td>@Alexissnicolee_  hey give me your fucking mon...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>@MajorPaulSmyth would love to go but got to be...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>Secular fake atheist come to give u lecture on...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   cyberbullying_type\n",
       "40362  @Goree_JuhssGuns hahaha he ain't even worth my...            ethnicity\n",
       "15019  RT @hsaymssik: Sucks to have the smile wiped o...               gender\n",
       "46321  Just a reminder, it's absolutely disgusting to...            ethnicity\n",
       "23927  RT @BuzzFeedUK: When you accidentally open you...  other_cyberbullying\n",
       "1640               Loving the look of the fritters! #mkr    not_cyberbullying\n",
       "46681  Has 2 interesting events last year involving r...            ethnicity\n",
       "40139              NIGGERS is the real way you dumb fuck            ethnicity\n",
       "12668  things that AREN'T jokes - rape - sexism - rac...               gender\n",
       "36459  You were bullied? If so, I’m sorry that happen...                  age\n",
       "8442   Haha did you watch big brother?, “Zankie” was ...               gender\n",
       "12216  RT @JusFappinAround: @nflis that a female refe...               gender\n",
       "13293  men when they think saying any slur making rac...               gender\n",
       "27948  @GCMayhem Sorry, I don't converse with people ...  other_cyberbullying\n",
       "8317   &#128514;&#128514;&#128514;&#128514;&#128514;&...               gender\n",
       "21391  So you are saying any Muslim scholar who say e...             religion\n",
       "34298  I hear you on that! When you’re a teenager. Yo...                  age\n",
       "14068  Long ago, I asked some white gay men on our lu...               gender\n",
       "22587  @Abu_Baraa1 @Myself_00001 Now if we can only g...             religion\n",
       "42812  “@applecheeksxo: Wait what RT @tayyoung_: FUCK...            ethnicity\n",
       "30506                        sorry i'm not humble, dick.  other_cyberbullying\n",
       "33581  if i went to school with draco idc if he was t...                  age\n",
       "17394  @discerningmumin That's idiotic. Not holding a...             religion\n",
       "45731  DEANGELO WHY ARE YOU SUCH A SORRY SACK OF DUMB...            ethnicity\n",
       "14828  @cervixseasoning Just a joke. I thought gay ra...               gender\n",
       "25315  A lot of indie game devs remind me a LOT of FO...  other_cyberbullying\n",
       "15821  The missus loves #MKR if I have to watch five ...               gender\n",
       "45951  RT @boycottdbanj: @ChyyChy fuck you coon.WE CA...            ethnicity\n",
       "24379  @Alexissnicolee_  hey give me your fucking mon...  other_cyberbullying\n",
       "5275   @MajorPaulSmyth would love to go but got to be...    not_cyberbullying\n",
       "21081  Secular fake atheist come to give u lecture on...             religion"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cyberbullying_tweets.csv\").sample(frac=0.2, random_state=42)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4b820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
