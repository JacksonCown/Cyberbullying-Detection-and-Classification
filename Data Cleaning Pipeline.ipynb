{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661a9f63",
   "metadata": {},
   "source": [
    "This iPython notebook is dedicated to maintaining the cleaning pipeline for the text data. It contains tools and for processing each tweet as well as a function to generate the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to \n",
    "#!python -m spacy download en_core_web_md\n",
    "#!pip3 install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb3531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# NLTK Resources\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "import spacy\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "# Init Tools\n",
    "nlp = spacy.load('en_core_web_md', disable =['ner', 'parser', 'textcat'])\n",
    "# This leaves #words and @words untouched\n",
    "tokenizer = RegexpTokenizer(r\"(@\\w+|#\\w+|\\w+)\") # r-string literal\n",
    "# Lemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb2e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: \n",
      "i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\n"
     ]
    }
   ],
   "source": [
    "# Checking the NLTK Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stpWords = stopwords.words('english')\n",
    "print(\"Stop words: \")\n",
    "print(', '.join(stpWords))\n",
    "\n",
    "# List of Parts-of-Speech to remove\n",
    "stpPOS = ['d', 'x', '.', 'p', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bcb99",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b25a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the username's mentioned in a tweet\n",
    "def extract_usernames(text):\n",
    "    usernames = re.findall(r'@\\s*\\w+', text)\n",
    "    return \" \".join([user.strip('@') for user in usernames])\n",
    "\n",
    "# @user subbing function to apply to whole text column\n",
    "def sub_usernames(text):\n",
    "    cleaned = re.sub(r'@\\w+', '@username', text)\n",
    "    return cleaned\n",
    "    #return re.sub(r'@', '', cleaned)\n",
    "\n",
    "# Extracts the Hashtags and returns them as a single string\n",
    "def extract_hashtags(text):\n",
    "    hashtags = re.findall(r'#\\s*\\w+', text)\n",
    "    return \" \".join([tag.strip('#') for tag in hashtags])\n",
    "\n",
    "# Replaces any Hashtag with a generic #hashtag\n",
    "def remove_hashtags(text):\n",
    "    #cleaned = re.sub(r'#\\w+', '#hashtag', text) # Temp Change for NB training\n",
    "    cleaned = re.sub(r'#\\w+', '', text)\n",
    "    return cleaned\n",
    "    #return re.sub(r'#', '', cleaned)\n",
    "\n",
    "# Remove Punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    cleaned = tokenizer.tokenize(text)\n",
    "    #return ' '.join(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "# Helper function to prep the POS tagged words for lemmatizing\n",
    "# Code sampled from my NLTK Repo:\n",
    "# https://github.com/JacksonCown/Workshop-NLTK/blob/main/NLTK%20Short%20Examples-checkpoint.ipynb\n",
    "def pos_for_lem(words_pos):\n",
    "    newTags = []\n",
    "    for tup in words_pos:\n",
    "        converted = tup[1][0].lower() # Takes the first letter of the lowercase tag\n",
    "        newTup = (tup[0], converted)\n",
    "        newTags.append(newTup)\n",
    "    return newTags\n",
    "\n",
    "# Tag tweet parts-of-speech for lemmatizer\n",
    "# Input tweet must be tokenized list\n",
    "# Must be done before removing stop words\n",
    "def tag_pos(words):\n",
    "    words = nltk.pos_tag(words, tagset='universal') # universal tagset\n",
    "    # Process with helper fuction before returning\n",
    "    return pos_for_lem(words)\n",
    "\n",
    "# Remove Spacy Stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# This uses NLTK stop words, it will be much slower for now but it solves\n",
    "# an issues I am having where \"#hashtag\" --> \"# hashtag\"\n",
    "# Remove stop words after pos tagging\n",
    "def remove_stopwords_pos(text_pos):\n",
    "    cleaned_list = []\n",
    "    for word in text_pos:\n",
    "        word_lower = word[0].lower()\n",
    "        # This can be improved\n",
    "        if word_lower not in stpWords and word[1] not in stpPOS:\n",
    "            cleaned_list.append((word_lower, word[1]))\n",
    "    return cleaned_list\n",
    "\n",
    "# Lemmatize the part of speech tagged tweet\n",
    "def lemmatize_tweet(text_pos):\n",
    "    lemmatized = [lemm.lemmatize(word, pos) for word, pos in text_pos]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Detect Language of Tweet Text\n",
    "def detect_language(text):\n",
    "    # Attempt to Detect Tweet Language\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return \"lang_error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae52c",
   "metadata": {},
   "source": [
    "#### Execution Order Per Tweet:\n",
    "\n",
    "1. Clean Usernames\n",
    "2. Clean Hashtags\n",
    "3. Clean Punctuation\n",
    "4. Tag Parts-Of-Speech\n",
    "5. Clean Stop Words\n",
    "6. Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c61bd",
   "metadata": {},
   "source": [
    "#### Main Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f6ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column of labels for language detected\n",
    "def df_lang_detect(df):\n",
    "    df[\"lang\"] = df[\"tweet_text\"].apply(detect_language)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the dataframe tweet text\n",
    "# Also create a new column containing the extracted usernames\n",
    "def df_clean_usernames(df):\n",
    "    df[\"mentioned_users\"] = df[\"tweet_text\"].apply(extract_usernames)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(sub_usernames)\n",
    "    return df\n",
    "\n",
    "# Remove all hashtags from the dataframe tweet text\n",
    "# Also create a new column containing the extracted hashtags\n",
    "def df_clean_hashtags(df):\n",
    "    df[\"hashtags\"] = df[\"tweet_text\"].apply(extract_hashtags)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_hashtags)\n",
    "    return df\n",
    "\n",
    "# Remove all punctuation from the data frame tweet text\n",
    "def df_clean_punctuation(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_punctuation)\n",
    "    return df\n",
    "\n",
    "# Tag all Parts-of-Speech in the tweet column\n",
    "def df_pos_tag(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(tag_pos)\n",
    "    return df\n",
    "\n",
    "# Remove all stop words from the data frame tweet text.\n",
    "# Much slower using remove_stopwords_pos than remove_stopwords\n",
    "# while I am trying to fix issue with hashtag tokenization\n",
    "def df_clean_stopwords(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(remove_stopwords_pos)\n",
    "    return df\n",
    "\n",
    "# Lemmatize the tweet text column of the dataframe\n",
    "# Tag parts of speech and remove stopwords first\n",
    "def df_lemmatize(df):\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lemmatize_tweet)\n",
    "    return df\n",
    "\n",
    "# Entire Cleaning Pipeline\n",
    "def df_clean(df):\n",
    "    print(\"Cleaning DataFrame: Starting\\n\")\n",
    "    df = df_lang_detect(df)\n",
    "    print(\"Language Filtering: Complete\")\n",
    "    df = df_clean_usernames(df)\n",
    "    print(\"Substitute Usernames: Complete\")\n",
    "    df = df_clean_hashtags(df)\n",
    "    print(\"Clean Hashtags: Complete\")\n",
    "    df = df_clean_punctuation(df)\n",
    "    print(\"Clean Punctuation: Complete\")\n",
    "    df = df_pos_tag(df)\n",
    "    print(\"Tag Parts-of-Speech: Complete\")\n",
    "    df = df_clean_stopwords(df)\n",
    "    print(\"Remove Stop Words: Complete\")\n",
    "    df = df_lemmatize(df)\n",
    "    print(\"Lemmatize Text: Complete\")\n",
    "    # Filter To English Samples\n",
    "    return df[df[\"lang\"] == \"en\"]\n",
    "\n",
    "# Function to call the cleaning pipeline and write the\n",
    "# final dataset to memory\n",
    "def pipeline(path=\"data/cleaned/cleaned_lemmatized_english.csv\"):\n",
    "    df = pd.read_csv(\"data/cyberbullying_tweets.csv\")\n",
    "    df = df_clean(df)\n",
    "    df.to_csv(path)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6e8b4",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0558eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: Starting\n",
      "\n",
      "Language Filtering: Complete\n",
      "Substitute Usernames: Complete\n",
      "Clean Hashtags: Complete\n",
      "Clean Punctuation: Complete\n",
      "Tag Parts-of-Speech: Complete\n",
      "Remove Stop Words: Complete\n",
      "Lemmatize Text: Complete\n",
      "data/cleaned/nohashtag_cleaned_lemmatized_english.csv\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to generate entire dataset\n",
    "#pipeline\n",
    "pipeline(\"data/cleaned/nohashtag_cleaned_lemmatized_english.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075af2ed",
   "metadata": {},
   "source": [
    "### Testing on 30 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a6064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Raja5aab @Quickieleaks Yes, the test of god i...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Itu sekolah ya bukan tempat bully! Ga jauh kay...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karma. I hope it bites Kat on the butt. She is...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@stockputout everything but mostly my priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rebecca Black Drops Out of School Due to Bully...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@Jord_Is_Dead http://t.co/UsQInYW5Gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Bully flushes on KD http://twitvid.com/A2TNP</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ughhhh #MKR</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @Kurdsnews: Turkish state has killed 241 ch...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Love that the best response to the hotcakes th...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@sarinhacoral @Victor_Maggi tadinhu de mim , s...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@0xabad1dea @kelseytheodore2 twitter is basica...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Best pick up line? Hi, you're cute... ?: I lov...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Now I gotta walk to classss?! I officially hat...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@halalcunty @biebervalue @liamxkiwi @greenline...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I still have Jack, Amsterdam, Ciroc, Crown, Bu...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@scottyswaggod men are the ones that are going...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Wishing my arena partner was on. &amp;gt;.&amp;gt;  Re...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Nation: Bullying | Turn to 10 http:/...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@sand_dejesus Isso √© bullying! @O_Patriarca</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@gcarothers eek. i can't stand split keyboards...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@MaxBlumenthal @cpassevant @anadumitrescu13 Po...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>You know there are people out there who like @...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type\n",
       "0   In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1   Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2   @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3   @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4   @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "5   @Raja5aab @Quickieleaks Yes, the test of god i...  not_cyberbullying\n",
       "6   Itu sekolah ya bukan tempat bully! Ga jauh kay...  not_cyberbullying\n",
       "7   Karma. I hope it bites Kat on the butt. She is...  not_cyberbullying\n",
       "8        @stockputout everything but mostly my priest  not_cyberbullying\n",
       "9   Rebecca Black Drops Out of School Due to Bully...  not_cyberbullying\n",
       "10               @Jord_Is_Dead http://t.co/UsQInYW5Gn  not_cyberbullying\n",
       "11   The Bully flushes on KD http://twitvid.com/A2TNP  not_cyberbullying\n",
       "12                                        Ughhhh #MKR  not_cyberbullying\n",
       "13  RT @Kurdsnews: Turkish state has killed 241 ch...  not_cyberbullying\n",
       "14  Love that the best response to the hotcakes th...  not_cyberbullying\n",
       "15  @yasmimcaci @Bferrarii PAREM DE FAZER BULLYING...  not_cyberbullying\n",
       "16  @sarinhacoral @Victor_Maggi tadinhu de mim , s...  not_cyberbullying\n",
       "17  @0xabad1dea @kelseytheodore2 twitter is basica...  not_cyberbullying\n",
       "18  Best pick up line? Hi, you're cute... ?: I lov...  not_cyberbullying\n",
       "19  Now I gotta walk to classss?! I officially hat...  not_cyberbullying\n",
       "20  @halalcunty @biebervalue @liamxkiwi @greenline...  not_cyberbullying\n",
       "21  Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...  not_cyberbullying\n",
       "22  I still have Jack, Amsterdam, Ciroc, Crown, Bu...  not_cyberbullying\n",
       "23  @scottyswaggod men are the ones that are going...  not_cyberbullying\n",
       "24  Wishing my arena partner was on. &gt;.&gt;  Re...  not_cyberbullying\n",
       "25  Education Nation: Bullying | Turn to 10 http:/...  not_cyberbullying\n",
       "26        @sand_dejesus Isso √© bullying! @O_Patriarca  not_cyberbullying\n",
       "27  @gcarothers eek. i can't stand split keyboards...  not_cyberbullying\n",
       "28  @MaxBlumenthal @cpassevant @anadumitrescu13 Po...  not_cyberbullying\n",
       "29  You know there are people out there who like @...  not_cyberbullying"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncleaned Dataset\n",
    "# Testing Entire Process on 30 samples\n",
    "df = pd.read_csv(\"data/cyberbullying_tweets.csv\").iloc[0:30]\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7cdbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: Starting\n",
      "\n",
      "Language Filtering: Complete\n",
      "Substitute Usernames: Complete\n",
      "Clean Hashtags: Complete\n",
      "Clean Punctuation: Complete\n",
      "Tag Parts-of-Speech: Complete\n",
      "Remove Stop Words: Complete\n",
      "Lemmatize Text: Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>mentioned_users</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word food crapilicious</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@username classy whore red velvet cupcake</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@username meh p thanks head concern angry dude...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@username isi account pretend kurdish account ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@username @username yes test god good bad indi...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@username everything mostly priest</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rebecca black drop school due bullying</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@username http co usqinyw5gn</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bully flush kd http twitvid com a2tnp</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rt @username turkish state kill 241 child last...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>love best response hotcake manage film non com...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@username @username twitter basically angry le...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>best pick line hi cute love people call james ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gotta walk class officially hate stupid bus sy...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@username @username @username @username @usern...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username username username username username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>still jack amsterdam ciroc crown bud light lim...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@username men one go push real change one powe...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>education nation bully turn 10 http co sxtiwtp</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@username isso √© bullying @username</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@username @username @username post hebdo lol e...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username username username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>know people like @username listen old school</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>en</td>\n",
       "      <td>username</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text cyberbullying_type lang  \\\n",
       "0                              word food crapilicious  not_cyberbullying   en   \n",
       "1                                               white  not_cyberbullying   en   \n",
       "2           @username classy whore red velvet cupcake  not_cyberbullying   en   \n",
       "3   @username meh p thanks head concern angry dude...  not_cyberbullying   en   \n",
       "4   @username isi account pretend kurdish account ...  not_cyberbullying   en   \n",
       "5   @username @username yes test god good bad indi...  not_cyberbullying   en   \n",
       "8                  @username everything mostly priest  not_cyberbullying   en   \n",
       "9              rebecca black drop school due bullying  not_cyberbullying   en   \n",
       "10                       @username http co usqinyw5gn  not_cyberbullying   en   \n",
       "11              bully flush kd http twitvid com a2tnp  not_cyberbullying   en   \n",
       "13  rt @username turkish state kill 241 child last...  not_cyberbullying   en   \n",
       "14  love best response hotcake manage film non com...  not_cyberbullying   en   \n",
       "17  @username @username twitter basically angry le...  not_cyberbullying   en   \n",
       "18  best pick line hi cute love people call james ...  not_cyberbullying   en   \n",
       "19  gotta walk class officially hate stupid bus sy...  not_cyberbullying   en   \n",
       "20  @username @username @username @username @usern...  not_cyberbullying   en   \n",
       "22  still jack amsterdam ciroc crown bud light lim...  not_cyberbullying   en   \n",
       "23  @username men one go push real change one powe...  not_cyberbullying   en   \n",
       "25     education nation bully turn 10 http co sxtiwtp  not_cyberbullying   en   \n",
       "26                @username isso √© bullying @username  not_cyberbullying   en   \n",
       "28  @username @username @username post hebdo lol e...  not_cyberbullying   en   \n",
       "29       know people like @username listen old school  not_cyberbullying   en   \n",
       "\n",
       "                                 mentioned_users hashtags  \n",
       "0                                                          \n",
       "1                                                          \n",
       "2                                       username           \n",
       "3                                       username           \n",
       "4                                       username           \n",
       "5                              username username           \n",
       "8                                       username           \n",
       "9                                                          \n",
       "10                                      username           \n",
       "11                                                         \n",
       "13                                      username           \n",
       "14                                                         \n",
       "17                             username username           \n",
       "18                                                         \n",
       "19                                                         \n",
       "20  username username username username username           \n",
       "22                                                         \n",
       "23                                      username           \n",
       "25                                                         \n",
       "26                             username username           \n",
       "28                    username username username           \n",
       "29                                      username           "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test sample clean\n",
    "df = df_clean(df)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679939ca",
   "metadata": {},
   "source": [
    "### Testing Functions for individual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5be4c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Tweet:  Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Sub Usernames X1:  Bruh what bruh @username #bruh: of to did  the #ruh - #hello # @username. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Hashtags X2:  Bruh what bruh @username #hashtag: of to did  the #hashtag - #hashtag # @username. I am is wowow, #hashtag, a videos,: wtf !!! ????? ]]]]] \n",
      "\n",
      "Remove Punctuation X3:  ['Bruh', 'what', 'bruh', '@username', '#hashtag', 'of', 'to', 'did', 'the', '#hashtag', '#hashtag', '@username', 'I', 'am', 'is', 'wowow', '#hashtag', 'a', 'videos', 'wtf'] \n",
      "\n",
      "Tag Parts-of-Speech X4:  [('Bruh', 'n'), ('what', 'p'), ('bruh', 'v'), ('@username', 'a'), ('#hashtag', 'n'), ('of', 'a'), ('to', 'p'), ('did', 'v'), ('the', 'd'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('@username', 'n'), ('I', 'p'), ('am', 'v'), ('is', 'v'), ('wowow', 'a'), ('#hashtag', 'a'), ('a', 'd'), ('videos', 'n'), ('wtf', 'n')] \n",
      "\n",
      "Remove Stop Words X5:  [('bruh', 'n'), ('bruh', 'v'), ('@username', 'a'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('#hashtag', 'n'), ('@username', 'n'), ('wowow', 'a'), ('#hashtag', 'a'), ('videos', 'n'), ('wtf', 'n')] \n",
      "\n",
      "Lemmatize Tweet X6:  bruh bruh @username #hashtag #hashtag #hashtag @username wowow #hashtag video wtf \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing For Singular Tweet Cleaning Fuctions\n",
    "# Very messy sample tweet to clean\n",
    "fake_tweet = \"Bruh what bruh @bruh #bruh: of to did  the #ruh - #hello # @hello. I am is wowow, #gaming, a videos,: wtf !!! ????? ]]]]]\"\n",
    "# Very simple pipeline tester\n",
    "def test_pipeline(tweet=fake_tweet):\n",
    "    print(\"Source Tweet: \", tweet, \"\\n\")\n",
    "    x1 = sub_usernames(fake_tweet)\n",
    "    print(\"Sub Usernames X1: \",x1, \"\\n\")\n",
    "    x2 = remove_hashtags(x1)\n",
    "    print(\"Remove Hashtags X2: \",x2, \"\\n\")\n",
    "    x3 = remove_punctuation(x2)\n",
    "    print(\"Remove Punctuation X3: \",x3, \"\\n\")\n",
    "    x4 = tag_pos(x3) # universal tagset\n",
    "    print(\"Tag Parts-of-Speech X4: \",x4, \"\\n\")\n",
    "    x5 = remove_stopwords_pos(x4)\n",
    "    print(\"Remove Stop Words X5: \", x5, \"\\n\")\n",
    "    x6 = lemmatize_tweet(x5)\n",
    "    print(\"Lemmatize Tweet X6: \", x6, \"\\n\")\n",
    "\n",
    "test_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
